<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shenghao Li</title>
  
  <meta name="author" content="Shenghao Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
	<!-- <link rel="icon" href="images/ShenghaoLi.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shenghao Li</name>
              </p>
              <p>
                I just graduated with a Ph.D. from <a href="https://www.sjtu.edu.cn/">SJTU</a> in Shanghai, where I work on 3D Vision, Local Feature Learning, and Scene Representation and Understanding. I was advised by <a href="https://automation.sjtu.edu.cn/Qun-Fei">Prof. Qunfei Zhao</a>. I am looking for opportunities a research scientist/engineer or a postdoctoral associate.
              </p>
              <p>
                I did my MS and BS in Mechatronics at Intelligent Robot Institute, part of the <a href="https://mech.ecust.edu.cn/main.htm">School of Mechanical and Power Engineering</a> at <a href="https://www.ecust.edu.cn/">ECUST</a>. My adviser was <a href="https://mech.ecust.edu.cn/2019/0516/c11227a90188/page.htm">Prof. Shuang Liu</a>, and I worked on several projects in computer vision and robotics, including VSLAM, object detection, object tracking, etc.
              </p>
              <p>
                From 2021 to 2023, I worked as an intern at MiniMax in the avatar group. I primarily design computer vision algorithms for 3D animatable avatars and 2D generative characters. Currently, I am working on AIGC with large vision-language models. Additionally, I interned at <a href="https://www.qualcomm.com/home">Qualcomm</a> as an AI engineer for mobile computing and at <a href="http://www.oceanring.cn/">Oceanbotech</a> as a robot vision engineer. 
              </p>
              <p>
                Please feel free to email me to discuss computer vision and robotics.
              </p>
              <p style="text-align:center">
                <a href="mailto:lch94102@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/resume.pdf">CV</a> &nbsp/&nbsp
                <a href="data/ShenghaoLi-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ww3F7TgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/CVShenghaoLi">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shenghao-li-b73b73a4">Linkedin</a>
                <!-- <a href="https://twitter.com/CVShenghaoLi">Twitter</a> &nbsp/&nbsp -->
                <!-- <a href="https://github.com/Merical">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ShenghaoLi.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ShenghaoLi_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about scene reconstruction and representation from images or videos. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
        <tr onmouseout="nerfslam_stop()" onmouseover="nerfslam_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:top;">
            <div class="one">
            <div class="two" id='nerfslam_image'><video  width="160" muted autoplay loop>
            <source src="images/nerfslam.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/nerfslam.png' width="160">
            </div>
            <script type="text/javascript">
            function nerfslam_start() {
                document.getElementById('nerfslam_image').style.opacity = "1";
            }

            function nerfslam_stop() {
                document.getElementById('nerfslam_image').style.opacity = "0";
            }
            nerfslam_stop()
            </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://merical.github.io/onlinenerf">
            <papertitle>Representing Boundary-ambiguous Scene Online with Scale-encoded Cascaded Grid Distillation and Radiance Field Deblurring</papertitle>
            </a>
            <br>
            <strong>Shenghao Li</strong>,
            <a>Zeyang Xia</a>,
            <a>Qunfei Zhao*</a>
            <br>
            <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2023.
            <br>
            <a href="https://ieeexplore.ieee.org/document/10197499">paper</a>
            <a href="https://merical.github.io/onlinenerf">project page</a>
            <!-- / -->
            <!-- <a href="https://arxiv.org/abs/2209.14988">arXiv</a> -->
            <!-- / -->
            <p>We propose a novel online scene representation method to address the issues of unknown camera poses, boundary ambiguity, and observation noises (camera motion blur). The proposed method can produce sharp and compact representations of scenes under various shooting conditions.</p>
        </td>
        </tr>

        <tr bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:top">
            <img src="images/s2ld.png" width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/10159656">
            <papertitle>Sparse-to-Local-Dense Matching for Geometry-Guided Correspondence Estimation</papertitle>
            <br>
            <strong>Shenghao Li</strong>,
            <a>Qunfei Zhao*</a>
            <a>Zeyang Xia</a>,
            <br>
            <em>IEEE Transactions on Image Processing</em>, 2023.
            <br>
            <a href="https://ieeexplore.ieee.org/document/10159656">paper</a>
            <!-- <a href="https://merical.github.io/s2ld/">project page</a> -->
            <!-- / -->
            <!-- <a href="https://arxiv.org/abs/2209.14988">arXiv</a> -->
            <!-- / -->
            <p>We propose a novel sparse-to-local-dense (S2LD) matching method to conduct fully differentiable correspondence estimation with the prior from epipolar geometry.</p>
        </td>
        </tr>

        <tr onmouseout="ssfslam_stop()" onmouseover="ssfslam_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:top;">
            <div class="one">
            <div class="two" id='ssfslam_image'><video  width="160" muted autoplay loop>
            <source src="images/ssf-slam.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/ssf-slam.png' width="160">
            </div>
            <script type="text/javascript">
            function ssfslam_start() {
                document.getElementById('ssfslam_image').style.opacity = "1";
            }

            function ssfslam_stop() {
                document.getElementById('ssfslam_image').style.opacity = "0";
            }
            ssfslam_stop()
            </script>
        </td>

        <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/9444777/">
            <papertitle>Quantized Self-supervised Local Feature for Real-time Robot Indirect VSLAM</papertitle>
            </a>
            <br>
            <strong>Shenghao Li</strong>,
            <a>Shuang Liu*</a>
            <a>Qunfei Zhao</a>
            <a>Qiaoyang Xia</a>,
            <br>
            <em>IEEE/ASME Transactions on Mechatronics</em>, 2022.
            <br>
            <!-- <a href="https://merical.github.io/ssf-slam/">project page</a> -->
            <a href="https://ieeexplore.ieee.org/document/9444777/">paper</a>
            <!-- / -->
            <p>We propose a quantized self-supervised local feature for the indirect VSLAM to handle the environmental interference in robot localization tasks.</p>
        </td>
        </tr>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
            <img src="images/ssfeature.png" width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/9517409/"><papertitle>Self-supervised Feature Detection and Binary Description in Hamming Space for Mobile Platforms</papertitle></a>
            <br>
            <strong>Shenghao Li</strong>,
            <a>Guibao Zhang</a>,
            <a>Qunfei Zhao*</a>
            <br>
            <em>IEEE International Conference on Real-time Computing and Robotics (RCAR)</em>, 2021.
            <br>
            <!-- <a href="https://ssfeature.github.io/">project page</a>
            / -->
            <a href="https://ieeexplore.ieee.org/document/9517409/">paper</a>
            <!-- / -->
            <p>We proposes a single-input multi-output model for feature extraction and a descriptor quantization approach to embedding the features into Hamming space.</p>
        </td>
        </tr>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
            <img src="images/drug.png" width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://ieeexplore.ieee.org/document/9588239"><papertitle>Automatic Drug Box Recognition Based on Depth Camera</papertitle></a>
            <br>
            <a>Changzheng Zhang</a>,
            <a>Qiaoyang Xia</a>,
            <strong>Shenghao Li</strong>,
            <a>Simeng Zhong</a>,
            <a>Shuang Liu*</a>,
            <br>
            <em>International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)</em>, 2021.
            <br>
            <a href="https://ieeexplore.ieee.org/document/9588239">paper</a>
            <p>We propose an automatic drug box detection method based on the gemoetry and color priors of the drug boxes observed from an RGB-D camera.</p>
        </td>
        </tr>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
            <img src="images/tghm.png" width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://www.mdpi.com/1424-8220/20/2/490">
            <papertitle>Autonomous Exploration and Map Construction of a Mobile Robot Based on the TGHM Algorithm</papertitle>
            </a>
            <br>
            <a>Shuang Liu</a>,
            <strong>Shenghao Li</strong>,
            <a>Luchao Pang</a>,
            <a>Jiahao Hu</a>,
            <a>Haoyao Chen*</a>
            <br>
            <em>Sensors</em>, 2020.
            <br>
            <a href="https://www.mdpi.com/1424-8220/20/2/490">paper</a>
            <p>We proposes proposes an autonomous exploration and map construction method based on an incremental caching topology-grid hybrid map (TGHM).</p>
        </td>
        </tr>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
            <img src="images/container.jpg" width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <papertitle>Automatic Container Code Localization and Recognition via an Efficient Code Detector and Sequence Recognition</papertitle>
            <br>
            <strong>Shenghao Li</strong>,
            <a>Shuang Liu*</a>,
            <a>Qiaoyang Xia</a>,
            <a>Hui Wang</a>,
            <a>Haoyao Chen</a>,
            <br>
            <em>IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)</em>, 2019.
            <br>
            <a href="https://ieeexplore.ieee.org/document/8868819">paper</a>
            <p>We propose an automatic container code localization and recognition system via an efficient code detector and a sequence recognizer.</p>
        </td>
        </tr>
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 Stolen from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
